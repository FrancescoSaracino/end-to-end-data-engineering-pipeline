[English](README.md) | [Italiano](README.it.md)

# Pipeline End-to-End di Data Engineering

## ğŸ“Œ Panoramica

Lâ€™obiettivo Ã¨ progettare e realizzare un flusso completo di gestione dei dati, coprendo lâ€™intero ciclo di vita:
dallâ€™ingestione e trasformazione dei dati, fino allâ€™esposizione tramite API REST e alla visualizzazione tramite dashboard BI.

Il progetto Ã¨ pensato come dimostrazione pratica di competenze fondamentali di **data engineering**, integrazione dei dati e analisi.

---

## ğŸ—ï¸ Architettura del Progetto

La pipeline segue le seguenti fasi principali:

1. Ingestione di un dataset grezzo in formato CSV  
2. Trasformazione e analisi dei dati tramite Python  
3. Generazione di un dataset processato  
4. Caricamento dei dati in un database relazionale  
5. Esposizione dei dati tramite API REST  
6. Visualizzazione dei dati tramite Power BI  

---

## ğŸ”„ Data Ingestion & ETL

La prima fase del progetto utilizza un **dataset pubblico in formato CSV** (`raw_data.csv`), memorizzato localmente.

Utilizzando **Python** e un approccio di **programmazione orientata agli oggetti**, il dataset grezzo viene pulito,
trasformato e analizzato.

Librerie principali utilizzate:
- **pandas** per la manipolazione e trasformazione dei dati  
- **scipy** per le analisi statistiche  
- **matplotlib** e **seaborn** per la visualizzazione esplorativa  

AttivitÃ  principali:
- pulizia dei dati  
- trasformazioni (ad esempio conversione dei tipi di dato)  
- analisi esplorativa dei dati  

Al termine del processo ETL viene generato il file **`processed_data.csv`**.

> **Nota:** il caricamento del dataset processato nel database viene attualmente effettuato manualmente.  
> Per questo motivo la pipeline puÃ² essere considerata **semi-automatizzata**, con focus su trasformazione e analisi.

---

## ğŸ“Š Analisi Statistica

Sul dataset processato sono stati calcolati diversi **indici statistici descrittivi**  
e sono stati individuati **outlier** per le variabili quantitative.

Sono state inoltre condotte:
- **One-Way ANOVA**
- **Correlazione di Spearman**

per analizzare le relazioni tra specifiche variabili del dataset.

---

## ğŸ—„ï¸ Database & REST API

Il file **`processed_data.csv`** Ã¨ stato importato in un **database MySQL** tramite **phpMyAdmin**
in ambiente locale (**XAMPP**).

A partire da questo database Ã¨ stata sviluppata una **REST API** utilizzando **Flask (Python)**.

Lâ€™API espone diversi **endpoint GET** che interrogano direttamente il database e restituiscono i risultati in formato **JSON**.

La configurazione del database Ã¨ gestita tramite **variabili dâ€™ambiente (`.env`)**.
Le informazioni sensibili non sono versionate, seguendo le best practice backend.

Uno degli endpoint include anche una **semplice interfaccia frontend** per la consultazione dei dati.

---

## ğŸ“ˆ Data Visualization

A partire dal file **`processed_data.csv`** Ã¨ stata realizzata una **dashboard Power BI**
per visualizzare i principali insight del dataset tramite grafici e indicatori.

---

## ğŸ“‚ Struttura del Progetto

```text
project_root/
â”‚
â”œâ”€â”€ api/                 # Flask REST API
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ raw_data.csv
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ processed_data.csv
â”œâ”€â”€ etl/                 # ETL & analisi statistica
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ pipeline/
â”‚       â”œâ”€â”€ base_pipeline.py        # classe base
â”‚       â””â”€â”€ amazon_pipeline.py     # pipeline specifica (entry point)
â”œâ”€â”€ bi/
â”‚   â””â”€â”€ dashboard.pbix              # Dashboard Power BI
â””â”€â”€ db/
    â””â”€â”€ schema.sql
```
---

## â–¶ï¸ Come Eseguire il Progetto (Setup Locale)

### Prerequisiti
- Python 3.9 o superiore  
- MySQL installato e in esecuzione  
- Virtual environment (consigliato)

### 1. Clonare il repository
Clona il repository del progetto e spostati nella directory principale del progetto.

```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
```

### 2. Configurare le variabili dâ€™ambiente

Allâ€™interno della cartella `api`, creare il file `.env` partendo dallâ€™esempio fornito.

```bash
cp api/.env.example api/.env
```

Modificare il file `.env` se necessario (nome database, utente, porta, ecc.).

### 3. Installare le dipendenze dellâ€™API

Spostati nella cartella `api` e installa le dipendenze Python necessarie.

```bash
cd api
pip install -r requirements.txt
```

### 4. Inizializzare il database

Utilizzare lo script SQL presente nella cartella `database` per creare schema e tabelle.

File SQL: 
```text
database/schema.sql
```

### 5. Avviare lâ€™API Flask

Avvia lâ€™applicazione Flask dalla cartella `api`.

```bash
python app.py
```

Lâ€™API sarÃ  disponibile allâ€™indirizzo::
```
http://127.0.0.1:5000
```

### 6. Eseguire la pipeline ETL
Per eseguire il processo ETL e generare il dataset elaborato, accedi alla cartella etl e installa le dipendenze richieste:

```bash
cd etl
pip install -r requirements.txt
python pipeline/amazon_pipeline.py
```
---

## ğŸ¯ Obiettivo del Progetto

L'obiettivo del progetto Ã¨ di dimostrare la capacitÃ  di progettare e implementare una pipeline di data engineering end-to-end, applicando concetti fondamentali come:
- ingestione dei dati
- trasformazione e analisi statistica
- modellazione e interrogazione di database
- esposizione dei dati tramite API
- visualizzazione dei dati

---

## ğŸš€ Miglioramenti Futuri

- Automazione completa della fase di caricamento nel database  
- Containerizzazione tramite Docker  
- Controlli di qualitÃ  e validazione dei dati  
- Deploy dellâ€™API su infrastruttura cloud  

---

## ğŸ‘¤ Autore

Francesco Saracino  
Junior Data Engineer


